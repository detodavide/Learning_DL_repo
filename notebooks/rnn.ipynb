{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split\n",
    "from torch.nn.utils import rnn as rnn_utils\n",
    "from generators.data_generation import generate_sequences\n",
    "from model_managers.DeepLearningManager import DeepLearningManager\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_sequence(points, direction):\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(3, 2))\n",
    "    \n",
    "    # Plot the sequence of points\n",
    "    point_array = np.array(points)\n",
    "    ax.plot(point_array[:, 0], point_array[:, 1], marker='o', linestyle='-')\n",
    "    \n",
    "    # Plot direction arrow\n",
    "    if direction == 1:  # Clockwise\n",
    "        start_point = point_array[0]\n",
    "        end_point = point_array[-1]\n",
    "        dx = end_point[0] - start_point[0]\n",
    "        dy = end_point[1] - start_point[1]\n",
    "        ax.arrow(start_point[0], start_point[1], dx, dy, head_width=0.1, head_length=0.1, fc='k', ec='k')\n",
    "    elif direction == 0:  # Counterclockwise\n",
    "        start_point = point_array[-1]\n",
    "        end_point = point_array[0]\n",
    "        dx = end_point[0] - start_point[0]\n",
    "        dy = end_point[1] - start_point[1]\n",
    "        ax.arrow(start_point[0], start_point[1], dx, dy, head_width=0.1, head_length=0.1, fc='k', ec='k')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title('Sequence of Points with Direction')\n",
    "    \n",
    "    # Show plot\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points, directions = generate_sequences(n=128, seed=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(3):\n",
    "    plot_sequence(points[i], directions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "n_hidden_dim = 2\n",
    "\n",
    "torch.manual_seed(101)\n",
    "rnn_cell = nn.RNNCell(input_size=n_features, hidden_size=n_hidden_dim)\n",
    "rnn_state = rnn_cell.state_dict()\n",
    "rnn_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To understand the RNN architecture, we utilize states generated by nn.RNNCell. This allows us to build the architecture from scratch, beginning with linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the linear layers and get the generated parameters from the RNNCell\n",
    "linear_input = nn.Linear(n_features, n_hidden_dim)\n",
    "linear_hidden = nn.Linear(n_hidden_dim, n_hidden_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    linear_input.weight = nn.Parameter(rnn_state['weight_ih'])\n",
    "    linear_input.bias = nn.Parameter(rnn_state['bias_ih'])\n",
    "    linear_hidden.weight = nn.Parameter(rnn_state['weight_hh'])\n",
    "    linear_hidden.bias = nn.Parameter(rnn_state['bias_hh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial hidden state set to 0 with dims 1 x 2\n",
    "initial_hidden = torch.zeros(1, n_hidden_dim)\n",
    "initial_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now generate the first hidden state, this is a simple linear transformation without any activ func\n",
    "th = linear_hidden(initial_hidden)\n",
    "th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now take the first sequence with 4 points, 2 x 4\n",
    "X = torch.as_tensor(points[0]).float()\n",
    "X, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = linear_input(X[0:1])\n",
    "tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the linear transformations to replicate the RNN\n",
    "adds = th + tx\n",
    "# Then use the tanh activation function\n",
    "torch.tanh(adds)\n",
    "\n",
    "# What we get is the updated hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cell(X[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer RNN\n",
    "n_features = 2\n",
    "n_hidden_dim = 2\n",
    "\n",
    "torch.manual_seed(101)\n",
    "rnn_cell = nn.RNN(input_size=n_features, hidden_size=n_hidden_dim)\n",
    "rnn_state = rnn_cell.state_dict()\n",
    "\n",
    "# As you can see we have l0 added to the weights and biases that indicates the layer 0\n",
    "rnn_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Input Dimension\n",
    "In PyTorch, if you set the batch_first argument to True when using the nn.RNN class, it adjusts the expected input tensor layout to have the batch dimension first. Therefore, if batch_first is set to True, the input tensor should have dimensions (batch_size, sequence_length, input_size). This is useful for compatibility with certain data formats or personal preference in organizing data.\n",
    "\n",
    "However, by default, PyTorch's nn.RNN class assumes the sequence dimension comes first. So, if batch_first is not specified or set to False, the input tensor should have dimensions (sequence_length, batch_size, input_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.as_tensor(points[:3]).float()\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from B S F -> S B F\n",
    "permuted_batch = batch.permute(1,0,2)\n",
    "\n",
    "# RNN friendly dimensions: Sequence - batch - Features\n",
    "permuted_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch second\n",
    "torch.manual_seed(101)\n",
    "rnn = nn.RNN(input_size=n_features, hidden_size=n_hidden_dim)\n",
    "out, final_hidden = rnn(permuted_batch)\n",
    "out.shape, final_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or use batch_first argument\n",
    "torch.manual_seed(101)\n",
    "rnn = nn.RNN(input_size=n_features, batch_first=True ,hidden_size=n_hidden_dim)\n",
    "out, final_hidden = rnn(batch)\n",
    "out.shape, final_hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember that Datasets and Dataloaders have batch_number as first dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Layers stacked\n",
    "torch.manual_seed(101)\n",
    "rnn_stacked = nn.RNN(input_size=2, hidden_size=2, batch_first=True, num_layers=2)\n",
    "rnn_stacked_state = rnn_stacked.state_dict()\n",
    "rnn_stacked_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Bidirectional\n",
    "torch.manual_seed(101)\n",
    "rnn_bidirect = nn.RNN(input_size=2, hidden_size=2, batch_first=True, bidirectional=True)\n",
    "state = rnn_bidirect.state_dict()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create forward RNN and backward RNN and pass the parameters to the models\n",
    "torch.manual_seed(19)\n",
    "forward_rnn = nn.RNN(input_size=2, hidden_size=2, batch_first=True)\n",
    "backward_rnn = nn.RNN(input_size=2, hidden_size=2, batch_first=True)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k[:-8], v) for k, v in list(state.items())[4:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_rnn.load_state_dict(dict(list(state.items())[:4]))\n",
    "backward_rnn.load_state_dict(dict([(k[:-8], v) for k, v in list(state.items())[4:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the state dictionary into a list of key-value pairs and start from the fifth element\n",
    "state_items = list(state.items())[4:]\n",
    "\n",
    "# Initialize an empty dictionary to store the modified key-value pairs\n",
    "modified_state_dict = {}\n",
    "\n",
    "# Iterate over the key-value pairs obtained from the state dictionary\n",
    "for key, value in state_items:\n",
    "    # Modify the key to remove the '_reverse' suffix, assuming it's present\n",
    "    modified_key = key[:-8]  # Remove the last 8 characters from the key\n",
    "    # Add the modified key-value pair to the modified state dictionary\n",
    "    modified_state_dict[modified_key] = value\n",
    "\n",
    "# Convert the list of modified key-value pairs back into a dictionary\n",
    "modified_state_dict = dict(modified_state_dict)\n",
    "\n",
    "# Load the modified state dictionary into the backward RNN model\n",
    "backward_rnn.load_state_dict(modified_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(1,4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the sequence input to the backward_rnn\n",
    "x_rev = torch.flip(X, dims=[1])\n",
    "x_rev, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, h = forward_rnn(X)\n",
    "out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_rev, h_rev = backward_rnn(x_rev)\n",
    "out_rev, h_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([out, out_rev], dim=2), torch.cat([h, h_rev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_bidirect(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_points, test_directions = generate_sequences(seed=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "train_data = TensorDataset(torch.as_tensor(points).float(),\n",
    "                           torch.as_tensor(directions).view(-1,1).float())\n",
    "test_data = TensorDataset(torch.as_tensor(test_points).float(),\n",
    "                           torch.as_tensor(test_directions).view(-1,1).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check requires_grad for tensors in train_loader\n",
    "for x_batch, y_batch in train_loader:\n",
    "    print(\"x_batch requires_grad:\", x_batch.requires_grad)\n",
    "    print(\"y_batch requires_grad:\", y_batch.requires_grad)\n",
    "    break  # Print only the first batch\n",
    "\n",
    "# Check requires_grad for tensors in test_loader\n",
    "for x_batch, y_batch in test_loader:\n",
    "    print(\"x_batch requires_grad:\", x_batch.requires_grad)\n",
    "    print(\"y_batch requires_grad:\", y_batch.requires_grad)\n",
    "    break  # Print only the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.SimpleRNN import SquareModel\n",
    "from model_managers.GodoyStepByStep import StepByStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SquareModel(n_features=2, hidden_dim=2, n_outputs=1)\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager = DeepLearningManager(model, loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager.set_data_loaders(train_loader=train_loader, val_loader=test_loader)\n",
    "model_manager.train(n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = model_manager.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager.loader_apply(test_loader, model_manager.correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
